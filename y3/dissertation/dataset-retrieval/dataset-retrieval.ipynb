{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itoETOcJSTMR"
      },
      "source": [
        "## Retrieve Data from SourceGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYN1PLjU1Tod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98caa146-3623-4276-8a20-3415118582d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 62 kB 1.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -Uqqq requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpbB-JDoS7q6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import requests\n",
        "import os\n",
        "from pathlib import Path\n",
        "import concurrent.futures"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# directory to store files defaults to current directory\n",
        "OUTPUT_DIR = \".\""
      ],
      "metadata": {
        "id": "9Rti3vAP_e1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21wk-YgYBmGL"
      },
      "source": [
        "## Searching for test files using SourceGraph GraphQL API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYRDG0ARDApz"
      },
      "source": [
        "### Collect All Hits\n",
        "See [here](https://docs.sourcegraph.com/cli/how-tos/creating_an_access_token) for how to get your own sourcegraph API token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30hitopTka-W"
      },
      "outputs": [],
      "source": [
        "sg_token = \"ENTER SOURCEGRAPH_API_TOKEN\"\n",
        "\n",
        "payload = {\n",
        "  \"query\": \"query ($query: String!) {\\n  search(query: $query, version: V2) {\\n    results {\\n      results {\\n        __typename\\n        ... on FileMatch {\\n          ...FileMatchFields\\n        }\\n      }\\n      matchCount\\n      elapsedMilliseconds\\n    }\\n  }\\n}\\n\\nfragment FileMatchFields on FileMatch {\\n  repository {\\n    name\\n    url\\n  }\\n  file {\\n    name\\n    url\\n  }\\n}\",\n",
        "  \"variables\": {\n",
        "    \"query\": \"file:.*_test.py file:has.content(import unittest) count:1000000 fork:no lang:python\"\n",
        "  }\n",
        "}\n",
        "\n",
        "destination = f\"{OUTPUT_DIR}/datasets/raw_datasets.json\"\n",
        "path = Path(destination)\n",
        "\n",
        "if not(path.exists()) :\n",
        "  r = requests.post('https://sourcegraph.com/.api/graphql', json=payload, headers={'Authorization': f\"token {sg_token}\"}, stream=True)\n",
        "  status = r.status_code\n",
        "\n",
        "  if status == 200 :\n",
        "      with open(destination, 'w+') as file:\n",
        "        data = r.json()[\"data\"][\"search\"][\"results\"]\n",
        "        json.dump(data, file, indent = 4)\n",
        "  else:\n",
        "    print(f\"Status Code: {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5lgMhLzDbMe"
      },
      "source": [
        "### Filter Hits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_yWGLRzR8mT"
      },
      "source": [
        "#### By existence of Source File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLLTKcybr8ly",
        "outputId": "244f28a7-6060-4ef9-fb53-343300d72033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "731.44 % progess with source test mapping"
          ]
        }
      ],
      "source": [
        "def search_source(query, schema):\n",
        "  payload = {\n",
        "    \"query\": schema,\n",
        "    \"variables\": {\n",
        "      \"query\": query\n",
        "    }\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    r = requests.post('https://sourcegraph.com/.api/graphql', json=payload, headers={'Authorization': f\"token {sg_token}\"}, stream=True)\n",
        "  except:\n",
        "    r = requests.post('https://sourcegraph.com/.api/graphql', json=payload, headers={'Authorization': f\"token {sg_token}\"}, verify=False)\n",
        "\n",
        "  status = r.status_code\n",
        "\n",
        "  if status == 200 :\n",
        "    data = r.json()['data']['search']\n",
        "\n",
        "    if data == None:\n",
        "      return\n",
        "\n",
        "    results = data[\"results\"][\"results\"]\n",
        "\n",
        "    search_count = data[\"results\"]['matchCount']\n",
        "\n",
        "    if search_count == 1:\n",
        "      result = results[0]\n",
        "      return result\n",
        "      \n",
        "  else:\n",
        "    print(f\"\\rStatus Code: {status}\", end=\"\")\n",
        "    print(f\"\\r{r.headers}\", end=\"\")\n",
        "    if(status == 500):\n",
        "      retry = int(r.headers[\"Retry-After\"])\n",
        "      time.sleep(retry)\n",
        "      \n",
        "source_files = dict()\n",
        "source_test_map = dict()\n",
        "\n",
        "def map_source_to_test(test_result):\n",
        "  source_file = test_result[\"file\"][\"name\"].replace(\"_test\", \"\")\n",
        "  source_test_mapping = dict()\n",
        "  # If a source file is found in the list of successfully retrieved source files add the pair to a source-test map\n",
        "  if source_file in source_files.keys():\n",
        "    source_test_mapping[\"source\"] = source_files[source_file][\"source_file\"]\n",
        "    source_test_mapping[\"test\"] = test_result.pop(\"file\")\n",
        "\n",
        "    source_test_map[source_file] = source_test_mapping\n",
        "\n",
        "schema = \"query ($query: String!) {\\n  search(query: $query, version: V2) {\\n    results {\\n      results {\\n        __typename\\n        ... on FileMatch {\\n          ...FileMatchFields\\n        }\\n      }\\n      matchCount\\n    }\\n  }\\n}\\n\\nfragment FileMatchFields on FileMatch {\\n  file {\\n    name\\n    url\\n  }\\n}\"\n",
        "\n",
        "def create_source_test_map():\n",
        "  with open(destination, 'r+') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "    fileCount = 0\n",
        "    \n",
        "    sg_queries = []\n",
        "\n",
        "    test_results = data[\"results\"]\n",
        "\n",
        "    # Create queries to find corresponding source files for each test file and prep a dictionary\n",
        "    for test_result in test_results:\n",
        "      repo = test_result[\"repository\"][\"name\"]\n",
        "      test_file = test_result[\"file\"][\"name\"]\n",
        "      source_file = test_file.replace(\"_test\", \"\")\n",
        "      del test_result[\"__typename\"]\n",
        "      del test_result[\"repository\"]\n",
        "\n",
        "      sg_query = f\"repo:{repo} (file:.*/{source_file}) fork:no lang:python\"\n",
        "\n",
        "      sg_queries.append(sg_query)\n",
        "\n",
        "  # This should occur prior to calling create_source_test_map (indentation issues)      \n",
        "  with concurrent.futures.ThreadPoolExecutor(max_workers=300) as executor:\n",
        "    fileCount = 0\n",
        "    duplicates = 0\n",
        "    # Check if a source file can be found on the server using our query\n",
        "    for query in sg_queries:\n",
        "      result = executor.submit(search_source, query, schema).result()\n",
        "      \n",
        "      if result:\n",
        "        file_name = result[\"file\"][\"name\"]\n",
        "        if file_name not in source_files.keys():\n",
        "          source_files[file_name] = {\"source_file\": {\"name\" : file_name, \"url\" : result[\"file\"][\"url\"]}}\n",
        "          fileCount += 1\n",
        "          print(\"\\r{0:.2f} % progess with source file search\".format(100 * fileCount/(len(sg_queries)-duplicates)), end=\"\")\n",
        "        else:\n",
        "          duplicates += 1\n",
        "\n",
        "  with open(destination.replace(\"raw\", \"filtered\"), 'w+') as file:    \n",
        "      fileCount = 0\n",
        "      \n",
        "      source_file_count = len(source_files.keys())\n",
        "      # For each test file perform the source to test mapping\n",
        "      with concurrent.futures.ThreadPoolExecutor(max_workers=300) as executor:\n",
        "        for test_result in test_results:\n",
        "          executor.submit(map_source_to_test, test_result)\n",
        "          fileCount += 1\n",
        "          \n",
        "          print(\"\\r{0:.2f} % progess with source test mapping\".format(100 * fileCount/source_file_count), end=\"\")\n",
        "      \n",
        "      json.dump({\"results\": source_test_map}, file, indent = 4)\n",
        "          \n",
        "try:\n",
        "  with open(destination.replace(\"raw\", \"filtered\"), 'r+') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "except:\n",
        "  create_source_test_map()\n",
        "\n",
        "destination = destination.replace(\"raw\", \"filtered\")\n",
        "path = Path(destination)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brxmrVBkSJvI"
      },
      "source": [
        "#### By License and Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVaSrRR4SzLI",
        "outputId": "f8e69006-978c-457b-e338-d968921416b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1572 file(s) out of 14996"
          ]
        }
      ],
      "source": [
        "licenses = [\n",
        "    \"\\\"Licensed under the Apache License, Version 2.0\\\"\", # Apache License 2.0\n",
        "    \"\\\"Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>\\\"\", # GNU AGPLv3, GNU GPLv3, GNU LGPLv3\n",
        "    \"\\\"Mozilla Public License Version 2.0\\\"\", # Mozilla Public License Version 2.0,\n",
        "    \"\\\"Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\\\"\", # BSD License\n",
        "    \"\\\"MIT License\\\"\" # MIT License\n",
        "]\n",
        "\n",
        "with open(destination, 'r+') as file:\n",
        "    data = json.load(file)\n",
        "    results = data[\"results\"]\n",
        "    files = results.keys()\n",
        "\n",
        "fileCount = 0\n",
        "\n",
        "src_folder = destination.replace(\"filtered_datasets.json\", \"src\")\n",
        "\n",
        "if(not(Path(src_folder).exists())):\n",
        "  os.makedirs(src_folder)\n",
        "  os.makedirs(src_folder.replace(\"src\", \"test\"))\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=300) as executor:\n",
        "  for file in files:\n",
        "    repo = '/'.join(results[file][\"source\"][\"url\"].split('/')[:3]).replace('/', '', 1)\n",
        "    query = f\"repo:{repo}(file:{file} content:{' OR '.join(licenses)})\"\n",
        "    schema = schema.replace(\"url\", \"content\")\n",
        "    \n",
        "    result = executor.submit(search_source, query, schema).result()\n",
        "\n",
        "    if result:\n",
        "      source_content = result[\"file\"][\"content\"]\n",
        "\n",
        "      source_path = os.path.join(src_folder, file)\n",
        "      \n",
        "\n",
        "      test_file = file.replace(\".py\", \"_test.py\")\n",
        "      \n",
        "      query = f\"repo:{repo}(file:{test_file})\"\n",
        "\n",
        "      result = executor.submit(search_source, query, schema).result()\n",
        "\n",
        "      if result:\n",
        "      # Ensure both source and test are valid first\n",
        "        if (not(Path(source_path).exists())):\n",
        "          with open(source_path, \"w\") as f:\n",
        "            f.write(source_content)\n",
        "\n",
        "        test_content = result[\"file\"][\"content\"]\n",
        "\n",
        "        test_path = os.path.join(src_folder.replace('src', 'test'), test_file)\n",
        "        \n",
        "        if (not(Path(test_path).exists())):\n",
        "          with open(test_path, \"w\") as f:\n",
        "            f.write(test_content)\n",
        "\n",
        "        fileCount += 1\n",
        "        print(f\"\\r{fileCount} file(s) out of {len(files)}\", end=\"\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}