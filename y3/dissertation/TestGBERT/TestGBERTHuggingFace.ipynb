{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "r9XcSzBfuDFr",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FdRjQTDKLOg7"
   },
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:12:52.999279Z",
     "iopub.status.busy": "2023-05-09T01:12:52.998946Z",
     "iopub.status.idle": "2023-05-09T01:12:53.008777Z",
     "shell.execute_reply": "2023-05-09T01:12:53.007824Z",
     "shell.execute_reply.started": "2023-05-09T01:12:52.999248Z"
    },
    "id": "ZwUjdyIbLHG9"
   },
   "outputs": [],
   "source": [
    "TEST_BERT_DIR = \"/notebooks/TestBERT\"\n",
    "DATA_DIR = \"/datasets\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9aAwVzTCz2o-"
   },
   "source": [
    "## Env\n",
    "\"Go [here](https://docs.neptune.ai/setup/installation) to find out about setting up your own neptune project for experiment monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:12:53.011007Z",
     "iopub.status.busy": "2023-05-09T01:12:53.010648Z",
     "iopub.status.idle": "2023-05-09T01:12:53.015962Z",
     "shell.execute_reply": "2023-05-09T01:12:53.015013Z",
     "shell.execute_reply.started": "2023-05-09T01:12:53.010980Z"
    },
    "id": "ZNENffbBz2o_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"]=\"\"\n",
    "os.environ[\"NEPTUNE_PROJECT\"]=\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uVqdBRZaP5Dm"
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:12:53.021071Z",
     "iopub.status.busy": "2023-05-09T01:12:53.020603Z",
     "iopub.status.idle": "2023-05-09T01:14:59.436370Z",
     "shell.execute_reply": "2023-05-09T01:14:59.435429Z",
     "shell.execute_reply.started": "2023-05-09T01:12:53.021047Z"
    },
    "id": "nh5Hpp_O7TNu"
   },
   "outputs": [],
   "source": [
    "! pip install -Uqqq -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ4j5NlpntYk",
    "tags": []
   },
   "source": [
    "# TestBERT Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:14:59.438791Z",
     "iopub.status.busy": "2023-05-09T01:14:59.438530Z",
     "iopub.status.idle": "2023-05-09T01:15:01.889534Z",
     "shell.execute_reply": "2023-05-09T01:15:01.888258Z",
     "shell.execute_reply.started": "2023-05-09T01:14:59.438763Z"
    },
    "id": "IutjC8rZ2oRQ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import neptune\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader, RandomSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DMkzOLE-z2pE"
   },
   "source": [
    "## Preprocessing Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:15:01.891101Z",
     "iopub.status.busy": "2023-05-09T01:15:01.890687Z",
     "iopub.status.idle": "2023-05-09T01:15:01.897815Z",
     "shell.execute_reply": "2023-05-09T01:15:01.896889Z",
     "shell.execute_reply.started": "2023-05-09T01:15:01.891073Z"
    },
    "id": "3-zF1ukG661F"
   },
   "outputs": [],
   "source": [
    "# adapted from https://gist.github.com/phpdude/1ae6f19de213d66286c8183e9e3b9ec1\n",
    "def remove_doc_strings(src):\n",
    "    import ast, astunparse\n",
    "    try:\n",
    "        parsed = ast.parse(src)\n",
    "\n",
    "        for node in ast.walk(parsed):\n",
    "            # let's work only on functions & classes definitions\n",
    "            if not isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef, ast.Module)):\n",
    "                continue\n",
    "\n",
    "            if not len(node.body):\n",
    "                continue\n",
    "\n",
    "            if not isinstance(node.body[0], ast.Expr):\n",
    "                continue\n",
    "\n",
    "            if not hasattr(node.body[0], 'value') or not isinstance(node.body[0].value, ast.Str):\n",
    "                continue\n",
    "\n",
    "            node.body = node.body[1:]\n",
    "\n",
    "        return astunparse.unparse(parsed)\n",
    "    \n",
    "    except SyntaxError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:15:01.900449Z",
     "iopub.status.busy": "2023-05-09T01:15:01.899930Z",
     "iopub.status.idle": "2023-05-09T01:15:01.912329Z",
     "shell.execute_reply": "2023-05-09T01:15:01.911523Z",
     "shell.execute_reply.started": "2023-05-09T01:15:01.900424Z"
    },
    "id": "ZO44b9o2z2pG"
   },
   "outputs": [],
   "source": [
    "def load_and_save_dataset(args):\n",
    "    # adapted from https://huggingface.co/docs/datasets/process\n",
    "\n",
    "    def chunk_examples(examples, args):\n",
    "        \n",
    "        texts = []\n",
    "        file_name = []\n",
    "        \n",
    "        from transformers import AutoTokenizer\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "        \n",
    "        for idx, text in enumerate(examples[\"text\"]):\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            \n",
    "            new_texts = [\n",
    "                tokenizer.convert_tokens_to_string(tokens[i:i + args.max_length]) for i in range(0, len(tokens), args.max_length)\n",
    "            ]\n",
    "            \n",
    "            texts += new_texts\n",
    "            file_name += [examples[\"filename\"][idx]]*len(new_texts)\n",
    "            \n",
    "        return {\"text\": texts, \"filename\": file_name}\n",
    "    \n",
    "    # strip doc strings, split into chunks and add filename attribute\n",
    "    def format_docs(ds):\n",
    "        \n",
    "        if \"filename\" not in ds.features.keys():\n",
    "            ds = ds.add_column(name=\"filename\", column=ds.info.download_checksums.keys())\n",
    "        \n",
    "        ds = ds.map(\n",
    "            lambda example: {\"text\": remove_doc_strings(f\"{example['text']}\")}\n",
    "        ).filter(lambda example:example[\"text\"])\n",
    "            \n",
    "        ds = ds.map(lambda batch: chunk_examples(batch, args), batched=True, remove_columns=ds.column_names)\n",
    "        \n",
    "        return ds\n",
    "    \n",
    "    def merge_src_test(example):\n",
    "        t_file = example[\"filename\"].replace(\"/src\", \"/test\").replace(\".py\", \"_test.py\")\n",
    "        \n",
    "        # multiple chunks will match the below query so we select a random one\n",
    "        # should help simulate the imprtance of different parts of a source and test file\n",
    "        t_examples = test.filter(lambda example: example[\"filename\"] == t_file)\n",
    "        \n",
    "        # t_examples is None if the test could not be previously parsed\n",
    "        t_text = random.choice(t_examples)[\"text\"] if len(t_examples) > 0 else None\n",
    "        \n",
    "        return {\"source\": example[\"text\"], \"target\": t_text}\n",
    "    \n",
    "    DATASET_PATH = os.path.join(args.output_dir, \"datasets\")\n",
    "    \n",
    "    from datasets import load_dataset, load_from_disk\n",
    "    \n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        src_test = load_from_disk(DATASET_PATH)\n",
    "    else:\n",
    "        src = load_dataset(\"text\", data_files=os.path.join(f\"{DATA_DIR}/src\", \"**\"), sample_by=\"document\", split=\"train\")\n",
    "        test = load_dataset(\"text\", data_files=os.path.join(f\"{DATA_DIR}/test\", \"**\"), sample_by=\"document\", split=\"train\")\n",
    "\n",
    "        src = format_docs(src)\n",
    "        test = format_docs(test)\n",
    "\n",
    "        src_test = src.map(\n",
    "            merge_src_test, \n",
    "            remove_columns=[\"text\", \"filename\"], \n",
    "            num_proc=4\n",
    "        ).filter(lambda example : example[\"target\"])\n",
    "\n",
    "        src_test.save_to_disk(DATASET_PATH)\n",
    "            \n",
    "    return src_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:15:01.914040Z",
     "iopub.status.busy": "2023-05-09T01:15:01.913742Z",
     "iopub.status.idle": "2023-05-09T01:15:01.919881Z",
     "shell.execute_reply": "2023-05-09T01:15:01.918571Z",
     "shell.execute_reply.started": "2023-05-09T01:15:01.914015Z"
    },
    "id": "CXVUa2Vzz2pK"
   },
   "outputs": [],
   "source": [
    "def prep_dataset(dataset, tokenizer, test_size=0.2):\n",
    "    \n",
    "    def tokenize(example):\n",
    "        inputs = tokenizer(\n",
    "            example[\"source\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {**inputs, \"labels\": tokenizer(example[\"target\"], padding=\"max_length\", truncation=True)[\"input_ids\"]}\n",
    "        \n",
    "    dataset = dataset.map(\n",
    "        tokenize,\n",
    "        num_proc=4,\n",
    "        remove_columns=[\"source\", \"target\"]\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.train_test_split(test_size=test_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:15:01.921979Z",
     "iopub.status.busy": "2023-05-09T01:15:01.921633Z",
     "iopub.status.idle": "2023-05-09T01:15:01.930256Z",
     "shell.execute_reply": "2023-05-09T01:15:01.928660Z",
     "shell.execute_reply.started": "2023-05-09T01:15:01.921943Z"
    },
    "id": "wSDjfRZuz2pL"
   },
   "outputs": [],
   "source": [
    "def load_tokenizer(args, dataset):\n",
    "    \n",
    "    from itertools import chain\n",
    "    \n",
    "    def get_corpus():\n",
    "        for text in chain.from_iterable([dataset[col] for col in dataset.column_names]):\n",
    "            yield text\n",
    "            \n",
    "    from transformers import RobertaTokenizerFast, AutoConfig\n",
    "        \n",
    "    TOKENIZER_PATH = os.path.join(args.output_dir, \"tokenizer.json\")\n",
    "    \n",
    "    if not os.path.exists(TOKENIZER_PATH):\n",
    "\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained(\n",
    "            args.model_name_or_path, \n",
    "            model_max_length=args.max_length\n",
    "        )\n",
    "        \n",
    "        new_tokenizer = tokenizer.train_new_from_iterator(\n",
    "            get_corpus(), \n",
    "            AutoConfig.from_pretrained(args.model_name_or_path).vocab_size\n",
    "        )\n",
    "        \n",
    "        tokenizer.add_tokens(list(new_tokenizer.vocab.keys()))\n",
    "\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "    return RobertaTokenizerFast(tokenizer_file=TOKENIZER_PATH, model_max_length=args.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:15:01.931649Z",
     "iopub.status.busy": "2023-05-09T01:15:01.931413Z",
     "iopub.status.idle": "2023-05-09T01:15:01.939669Z",
     "shell.execute_reply": "2023-05-09T01:15:01.938841Z",
     "shell.execute_reply.started": "2023-05-09T01:15:01.931623Z"
    },
    "id": "GdrLybhpz2pN"
   },
   "outputs": [],
   "source": [
    "def init_new_embeddings(model, side, tokenizer, strategy=\"avg\"):\n",
    "        \n",
    "    # based on https://nlp.stanford.edu//~johnhew//vocab-expansion.html\n",
    "\n",
    "    num_tokens = len(tokenizer)\n",
    "    \n",
    "    from transformers import AutoConfig\n",
    "        \n",
    "    num_new_tokens = len(tokenizer) - AutoConfig.from_pretrained(model.encoder.name_or_path).vocab_size\n",
    "\n",
    "    if side == \"encoder\":\n",
    "        model.encoder.resize_token_embeddings(num_tokens)\n",
    "        weight_key = 'encoder.embeddings.word_embeddings.weight'\n",
    "\n",
    "    else:\n",
    "        model.decoder.resize_token_embeddings(num_tokens)\n",
    "        weight_key = 'decoder.roberta.embeddings.word_embeddings.weight'\n",
    "\n",
    "    params = model.state_dict()\n",
    "\n",
    "    embeddings = params[weight_key]\n",
    "    \n",
    "    pre_expansion_embeddings = embeddings[:-num_new_tokens,:]\n",
    "    mu = torch.mean(pre_expansion_embeddings, dim=0)\n",
    "    n = pre_expansion_embeddings.size()[0]\n",
    "    sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n\n",
    "    dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            mu, covariance_matrix=1e-5*sigma)\n",
    "\n",
    "    new_embeddings = torch.stack(tuple((dist.sample() for _ in range(num_new_tokens))), dim=0)\n",
    "    embeddings[-num_new_tokens:,:] = new_embeddings\n",
    "    \n",
    "    params[weight_key] = embeddings\n",
    "\n",
    "    model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:15:01.940777Z",
     "iopub.status.busy": "2023-05-09T01:15:01.940557Z",
     "iopub.status.idle": "2023-05-09T01:15:01.947137Z",
     "shell.execute_reply": "2023-05-09T01:15:01.946316Z",
     "shell.execute_reply.started": "2023-05-09T01:15:01.940756Z"
    },
    "id": "YrSLZiQLz2pO"
   },
   "outputs": [],
   "source": [
    "# from https://huggingface.co/docs/transformers/v4.18.0/en/performance#faster-optimizer\n",
    "def get_optimizer(model, training_args):\n",
    "    import bitsandbytes as bnb\n",
    "    from torch import nn\n",
    "    from transformers.trainer_pt_utils import get_parameter_names\n",
    "\n",
    "    decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "            \"weight_decay\": training_args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer_kwargs = {\n",
    "        \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
    "        \"eps\": training_args.adam_epsilon,\n",
    "    }\n",
    "    optimizer_kwargs[\"lr\"] = training_args.learning_rate\n",
    "    adam_bnb_optim = bnb.optim.Adam8bit(\n",
    "        optimizer_grouped_parameters,\n",
    "        betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "        eps=training_args.adam_epsilon,\n",
    "        lr=training_args.learning_rate,\n",
    "    )\n",
    "    \n",
    "    return adam_bnb_optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xT4n5ahHFHl4",
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:15:01.948484Z",
     "iopub.status.busy": "2023-05-09T01:15:01.948261Z",
     "iopub.status.idle": "2023-05-09T01:15:01.956249Z",
     "shell.execute_reply": "2023-05-09T01:15:01.955543Z",
     "shell.execute_reply.started": "2023-05-09T01:15:01.948462Z"
    },
    "id": "gPcHJi6DATE_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 3 recommended for finetuning by BERT paper but 10 in CodeBERT example: https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_VARIANT = \"microsoft/graphcodebert-base\"\n",
    "max_epochs = 5 \n",
    "\"\"\" 3 recommended for finetuning by BERT paper but 10 in CodeBERT example: https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N6vhtsIMz2pT"
   },
   "source": [
    "## Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:47:42.296291Z",
     "iopub.status.busy": "2023-05-09T01:47:42.295942Z",
     "iopub.status.idle": "2023-05-09T01:47:42.323242Z",
     "shell.execute_reply": "2023-05-09T01:47:42.322276Z",
     "shell.execute_reply.started": "2023-05-09T01:47:42.296265Z"
    },
    "id": "oFHkNpSK8fH_"
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    ## Required parameters  \n",
    "    parser.add_argument(\"--model_type\", default=None, type=str, required=True,\n",
    "                      help=\"Model type: e.g. roberta\")\n",
    "    parser.add_argument(\"--model_name_or_path\", default=None, type=str, required=True,\n",
    "                      help=\"Path to pre-trained model: e.g. roberta-base\" )\n",
    "    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
    "                      help=\"The output directory. Contains any cached files or outputs\")\n",
    "\n",
    "    ## Other parameters\n",
    "    parser.add_argument(\"--example_dir\", default=None, type=str, \n",
    "                      help=\"The example directory. Contains source and test .py files\")\n",
    "    parser.add_argument(\"--max_length\", default=512, type=int,\n",
    "                      help=\"The maximum total target sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
    "    parser.add_argument(\"--max_epochs\", default=-1, type=int,\n",
    "                        help=\"\")\n",
    "    parser.add_argument(\"--do_train\", action='store_true',\n",
    "                        help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--batch_size\", default=4, type=int,\n",
    "                      help=\"Batch size per GPU/CPU for training and evaluation.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
    "                      help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--num_beams\", default=10, type=int,\n",
    "                      help=\"The number of beams for beam search\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
    "                        help=\"Weight decay if we apply some.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
    "                      help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                      help=\"random seed for initialization\")\n",
    "    parser.add_argument('--subset_size', type=int, default=-1,\n",
    "                      help=\"Size of subset of dataset to use if not training on entirety\")\n",
    "    parser.add_argument('--report_to', type=str, default=\"none\",\n",
    "                      help=\"Where to log training data to\")\n",
    "    parser.add_argument('--warmup_ratio', type=str, default=0.05,\n",
    "                      help=\"Warmup ratio for linear scheduler; default based on 5 out of 90 epochs in original paper: https://arxiv.org/abs/1706.02677\")\n",
    "    \n",
    "    # print arguments\n",
    "    args = parser.parse_args() if len(args) == 0 else parser.parse_args(args)\n",
    "\n",
    "    # Dataset\n",
    "    \n",
    "    # first load dataset\n",
    "    dataset = load_and_save_dataset(args)\n",
    "    \n",
    "    # use dataset to initialise tokenizer\n",
    "    tokenizer = load_tokenizer(args, dataset)\n",
    "    tokenizer.bos_token = tokenizer.cls_token\n",
    "    tokenizer.eos_token = tokenizer.sep_token\n",
    "    \n",
    "    # use tokenizer to tranform and prep the dataset for training\n",
    "    dataset = prep_dataset(dataset, tokenizer)\n",
    "\n",
    "    # CodeBERT model config\n",
    "    \n",
    "    from transformers import EncoderDecoderModel, AutoModelForSeq2SeqLM\n",
    "    \n",
    "    ENCODER_DECODER_PATH = os.path.join(args.output_dir, \"encoder_decoder\")\n",
    "    \n",
    "    encoder_decoder = EncoderDecoderModel.from_encoder_decoder_pretrained(args.model_name_or_path, args.model_name_or_path, tie_encoder_decoder=True)\n",
    "    \n",
    "    # alter model encoder decoder embeddings using tokenizer\n",
    "    \n",
    "    for side in [\"encoder\", \"decoder\"]:\n",
    "        init_new_embeddings(encoder_decoder, side, tokenizer)\n",
    "    \n",
    "    encoder_decoder.save_pretrained(ENCODER_DECODER_PATH)\n",
    "    \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ENCODER_DECODER_PATH,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        decoder_start_token_id=tokenizer.bos_token_id,\n",
    "    )\n",
    "    \n",
    "    model.save_pretrained(args.output_dir)\n",
    "    \n",
    "    # Trainer\n",
    "    # args\n",
    "\n",
    "    from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, GenerationConfig\n",
    "    \n",
    "    gen_kwargs = {\n",
    "        'pad_token_id':tokenizer.pad_token_id,\n",
    "        'decoder_start_token_id':tokenizer.bos_token_id,\n",
    "        'max_new_tokens':args.max_length,\n",
    "        'min_new_tokens':args.max_length//2,\n",
    "        # beam-search multinomial sampling strategy\n",
    "        'do_sample':True,\n",
    "        'num_beams':args.num_beams,\n",
    "        'early_stopping':True,\n",
    "    }\n",
    "    \n",
    "    generation_config = GenerationConfig(**gen_kwargs)\n",
    "    \n",
    "    gradient_accumulation_steps = args.batch_size\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        fp16=True,\n",
    "        fp16_full_eval=True,\n",
    "        seed=args.seed,\n",
    "        save_total_limit=3,\n",
    "        report_to=args.report_to,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_bert\",\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        adam_epsilon=args.adam_epsilon,\n",
    "        num_train_epochs=args.max_epochs,\n",
    "        learning_rate=args.learning_rate,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        optim=\"adafactor\",\n",
    "        output_dir=f\"{args.output_dir}/trainer\",\n",
    "        per_device_train_batch_size=gradient_accumulation_steps//4,\n",
    "        per_device_eval_batch_size=gradient_accumulation_steps//4,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        gradient_checkpointing=True,\n",
    "        predict_with_generate=True,\n",
    "        generation_num_beams=args.num_beams,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    \n",
    "    # Data Collator\n",
    "    \n",
    "    from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        max_length=args.max_length,\n",
    "        padding=\"max_length\",\n",
    "        label_pad_token_id=model.config.pad_token_id,\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "    )\n",
    "    \n",
    "    # metrics\n",
    "    import evaluate\n",
    "    metric = evaluate.load(\"bertscore\")\n",
    "    \n",
    "    def decode_sequences(sequences):\n",
    "        return tokenizer.convert_tokens_to_string(tokenizer.batch_decode(sequences, skip_special_tokens=True))\n",
    "    \n",
    "    # adapted from https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation.py\n",
    "    \n",
    "    def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [[label.strip()] for label in labels]\n",
    "\n",
    "        return preds, labels\n",
    "\n",
    "    def compute_metrics(eval_preds, output_dir=args.output_dir):\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        # Replace -100s used for padding as we can't decode them\n",
    "        preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Some simple post-processing\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels, model_type=\"roberta-large\")\n",
    "        \n",
    "        return {\"eval_bert\": np.mean(result[\"f1\"]).item()}\n",
    "    \n",
    "    # training\n",
    "    \n",
    "    random.seed(a=args.seed)\n",
    "    \n",
    "    eval_ds = dataset[\"test\"]\n",
    "    eval_subset = eval_ds.select(random.sample(range(0, len(eval_ds)), len(eval_ds)//50))\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=eval_subset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=(get_optimizer(model, training_args), None)\n",
    "    )\n",
    "    \n",
    "    resume = len(os.listdir(os.path.join(args.output_dir, \"trainer\"))) > 0\n",
    "    \n",
    "    if args.do_train:\n",
    "        trainer.train(resume_from_checkpoint=resume)\n",
    "    else:\n",
    "        \n",
    "        inputs = dataset[\"train\"][0]\n",
    "        \n",
    "        text = decode_sequences(inputs[\"input_ids\"])\n",
    "        \n",
    "        preds, labels, metrics = trainer.predict([inputs])\n",
    "        \n",
    "        preds = decode_sequences(preds)\n",
    "        \n",
    "        expected = decode_sequences(labels)\n",
    "        \n",
    "        print(f\"Input: {text}\")\n",
    "        \n",
    "        print(f\"Prediction: {preds}\")\n",
    "        \n",
    "        print(f\"Expected: {expected}\")\n",
    "        \n",
    "        print(f\"Metrics: {metrics}\")                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TNCoz5ojMV3r"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:15:01.986859Z",
     "iopub.status.busy": "2023-05-09T01:15:01.986518Z",
     "iopub.status.idle": "2023-05-09T01:15:01.991459Z",
     "shell.execute_reply": "2023-05-09T01:15:01.990399Z",
     "shell.execute_reply.started": "2023-05-09T01:15:01.986832Z"
    },
    "id": "32nPVZni9z65",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main([\n",
    "#     \"--model_type\", \"roberta\", \n",
    "#     \"--model_name_or_path\", MODEL_VARIANT,\n",
    "#     \"--example_dir\", f\"{DATA_DIR}\",\n",
    "#     \"--output_dir\", f\"{TEST_BERT_DIR}\",\n",
    "#     \"--max_epochs\", f\"{max_epochs}\",\n",
    "#     \"--report_to\", \"neptune\",\n",
    "#     \"--do_train\"\n",
    "# ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "n0TfgZaW2PB2",
    "tags": []
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T01:49:53.530405Z",
     "iopub.status.busy": "2023-05-09T01:49:53.530028Z",
     "iopub.status.idle": "2023-05-09T01:50:41.518026Z",
     "shell.execute_reply": "2023-05-09T01:50:41.517082Z",
     "shell.execute_reply.started": "2023-05-09T01:49:53.530346Z"
    },
    "id": "_we9rUiW2Ouq"
   },
   "outputs": [],
   "source": [
    "# main([\"--model_type\", \"roberta\", \"--model_name_or_path\", MODEL_VARIANT, \"--output_dir\", f\"{TEST_BERT_DIR}\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qTaE9lmRz2pa"
   },
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-09T01:15:52.929475Z",
     "iopub.status.idle": "2023-05-09T01:15:52.929812Z",
     "shell.execute_reply": "2023-05-09T01:15:52.929660Z",
     "shell.execute_reply.started": "2023-05-09T01:15:52.929643Z"
    },
    "id": "5Whjay_pz2pb"
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
