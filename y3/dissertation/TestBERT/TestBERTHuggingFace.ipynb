{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9XcSzBfuDFr",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdRjQTDKLOg7"
   },
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T22:59:26.853164Z",
     "iopub.status.busy": "2023-05-06T22:59:26.852811Z",
     "iopub.status.idle": "2023-05-06T22:59:26.857294Z",
     "shell.execute_reply": "2023-05-06T22:59:26.856427Z",
     "shell.execute_reply.started": "2023-05-06T22:59:26.853136Z"
    },
    "id": "ZwUjdyIbLHG9"
   },
   "outputs": [],
   "source": [
    "TEST_BERT_DIR = \"/notebooks/TestBERT\"\n",
    "DATA_DIR = \"/datasets\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env\n",
    "Go [here](https://docs.neptune.ai/setup/installation) to find out about setting up your own neptune project for experiment monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T22:59:26.859493Z",
     "iopub.status.busy": "2023-05-06T22:59:26.859271Z",
     "iopub.status.idle": "2023-05-06T22:59:26.863613Z",
     "shell.execute_reply": "2023-05-06T22:59:26.862773Z",
     "shell.execute_reply.started": "2023-05-06T22:59:26.859471Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"NEPTUNE_API_TOKEN\"]=\"\"\n",
    "os.environ[\"NEPTUNE_PROJECT\"]=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVqdBRZaP5Dm"
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T22:59:26.864920Z",
     "iopub.status.busy": "2023-05-06T22:59:26.864660Z",
     "iopub.status.idle": "2023-05-06T22:59:30.918854Z",
     "shell.execute_reply": "2023-05-06T22:59:30.917596Z",
     "shell.execute_reply.started": "2023-05-06T22:59:26.864897Z"
    },
    "id": "nh5Hpp_O7TNu"
   },
   "outputs": [],
   "source": [
    "! pip install -qqq -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ4j5NlpntYk",
    "tags": []
   },
   "source": [
    "# TestBERT Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T22:59:30.920813Z",
     "iopub.status.busy": "2023-05-06T22:59:30.920505Z",
     "iopub.status.idle": "2023-05-06T22:59:32.443870Z",
     "shell.execute_reply": "2023-05-06T22:59:32.443033Z",
     "shell.execute_reply.started": "2023-05-06T22:59:30.920784Z"
    },
    "id": "IutjC8rZ2oRQ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import neptune\n",
    "from itertools import product\n",
    "from torch.utils.data import DataLoader, RandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T22:59:32.447833Z",
     "iopub.status.busy": "2023-05-06T22:59:32.447373Z",
     "iopub.status.idle": "2023-05-06T22:59:32.453734Z",
     "shell.execute_reply": "2023-05-06T22:59:32.452973Z",
     "shell.execute_reply.started": "2023-05-06T22:59:32.447805Z"
    },
    "id": "3-zF1ukG661F"
   },
   "outputs": [],
   "source": [
    "# adapted from https://gist.github.com/phpdude/1ae6f19de213d66286c8183e9e3b9ec1\n",
    "def remove_doc_strings(src):\n",
    "    import ast, astunparse\n",
    "    try:\n",
    "        parsed = ast.parse(src)\n",
    "\n",
    "        for node in ast.walk(parsed):\n",
    "            # let's work only on functions & classes definitions\n",
    "            if not isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef, ast.Module)):\n",
    "                continue\n",
    "\n",
    "            if not len(node.body):\n",
    "                continue\n",
    "\n",
    "            if not isinstance(node.body[0], ast.Expr):\n",
    "                continue\n",
    "\n",
    "            if not hasattr(node.body[0], 'value') or not isinstance(node.body[0].value, ast.Str):\n",
    "                continue\n",
    "\n",
    "            node.body = node.body[1:]\n",
    "\n",
    "        return astunparse.unparse(parsed)\n",
    "    \n",
    "    except SyntaxError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T22:59:32.455456Z",
     "iopub.status.busy": "2023-05-06T22:59:32.454757Z",
     "iopub.status.idle": "2023-05-06T22:59:32.466196Z",
     "shell.execute_reply": "2023-05-06T22:59:32.465371Z",
     "shell.execute_reply.started": "2023-05-06T22:59:32.455430Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_save_dataset(args):\n",
    "    # adapted from https://huggingface.co/docs/datasets/process\n",
    "\n",
    "    def chunk_examples(examples, args):\n",
    "        \n",
    "        texts = []\n",
    "        file_name = []\n",
    "        \n",
    "        from transformers import AutoTokenizer\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "        \n",
    "        for idx, text in enumerate(examples[\"text\"]):\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            \n",
    "            new_texts = [\n",
    "                tokenizer.convert_tokens_to_string(tokens[i:i + args.max_length]) for i in range(0, len(tokens), args.max_length)\n",
    "            ]\n",
    "            \n",
    "            texts += new_texts\n",
    "            file_name += [examples[\"filename\"][idx]]*len(new_texts)\n",
    "            \n",
    "        return {\"text\": texts, \"filename\": file_name}\n",
    "    \n",
    "    # strip doc strings, split into chunks and add filename attribute\n",
    "    def format_docs(ds):\n",
    "        \n",
    "        if \"filename\" not in ds.features.keys():\n",
    "            ds = ds.add_column(name=\"filename\", column=ds.info.download_checksums.keys())\n",
    "        \n",
    "        ds = ds.map(\n",
    "            lambda example: {\"text\": remove_doc_strings(f\"{example['text']}\")}\n",
    "        ).filter(lambda example:example[\"text\"])\n",
    "            \n",
    "        ds = ds.map(lambda batch: chunk_examples(batch, args), batched=True, remove_columns=ds.column_names)\n",
    "        \n",
    "        return ds\n",
    "    \n",
    "    def merge_src_test(example):\n",
    "        t_file = example[\"filename\"].replace(\"/src\", \"/test\").replace(\".py\", \"_test.py\")\n",
    "        \n",
    "        # multiple chunks will match the below query so we select a random one\n",
    "        # should help simulate the imprtance of different parts of a source and test file\n",
    "        t_examples = test.filter(lambda example: example[\"filename\"] == t_file)\n",
    "        \n",
    "        # t_examples is None if the test could not be previously parsed\n",
    "        t_text = random.choice(t_examples)[\"text\"] if len(t_examples) > 0 else None\n",
    "        \n",
    "        return {\"source\": example[\"text\"], \"target\": t_text}\n",
    "    \n",
    "    DATASET_PATH = os.path.join(args.output_dir, \"datasets\")\n",
    "    \n",
    "    from datasets import load_dataset, load_from_disk\n",
    "    \n",
    "    if os.path.exists(DATASET_PATH):\n",
    "        src_test = load_from_disk(DATASET_PATH)\n",
    "    else:\n",
    "        src = load_dataset(\"text\", data_files=os.path.join(f\"{DATA_DIR}/src\", \"**\"), sample_by=\"document\", split=\"train\")\n",
    "        test = load_dataset(\"text\", data_files=os.path.join(f\"{DATA_DIR}/test\", \"**\"), sample_by=\"document\", split=\"train\")\n",
    "\n",
    "        src = format_docs(src)\n",
    "        test = format_docs(test)\n",
    "\n",
    "        src_test = src.map(\n",
    "            merge_src_test, \n",
    "            remove_columns=[\"text\", \"filename\"], \n",
    "            num_proc=4\n",
    "        ).filter(lambda example : example[\"target\"])\n",
    "\n",
    "        src_test.save_to_disk(DATASET_PATH)\n",
    "            \n",
    "    return src_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T22:59:32.467309Z",
     "iopub.status.busy": "2023-05-06T22:59:32.467032Z",
     "iopub.status.idle": "2023-05-06T22:59:32.472685Z",
     "shell.execute_reply": "2023-05-06T22:59:32.472003Z",
     "shell.execute_reply.started": "2023-05-06T22:59:32.467285Z"
    }
   },
   "outputs": [],
   "source": [
    "def prep_dataset(dataset, tokenizer, test_size=0.2):\n",
    "    \n",
    "    def tokenize(example):\n",
    "        inputs = tokenizer(\n",
    "            example[\"source\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {**inputs, \"labels\": tokenizer(example[\"target\"], padding=\"max_length\", truncation=True)[\"input_ids\"]}\n",
    "        \n",
    "    dataset = dataset.map(\n",
    "        tokenize,\n",
    "        num_proc=4,\n",
    "        remove_columns=[\"source\", \"target\"]\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.train_test_split(test_size=test_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T22:59:32.473772Z",
     "iopub.status.busy": "2023-05-06T22:59:32.473536Z",
     "iopub.status.idle": "2023-05-06T22:59:32.479804Z",
     "shell.execute_reply": "2023-05-06T22:59:32.479138Z",
     "shell.execute_reply.started": "2023-05-06T22:59:32.473751Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_tokenizer(args, dataset):\n",
    "    \n",
    "    from itertools import chain\n",
    "    \n",
    "    def get_corpus():\n",
    "        for text in chain.from_iterable([dataset[col] for col in dataset.column_names]):\n",
    "            yield text\n",
    "            \n",
    "    from transformers import RobertaTokenizerFast, AutoConfig\n",
    "        \n",
    "    TOKENIZER_PATH = os.path.join(args.output_dir, \"tokenizer.json\")\n",
    "    \n",
    "    if not os.path.exists(TOKENIZER_PATH):\n",
    "\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained(\n",
    "            args.model_name_or_path, \n",
    "            model_max_length=args.max_length\n",
    "        )\n",
    "        \n",
    "        new_tokenizer = tokenizer.train_new_from_iterator(\n",
    "            get_corpus(), \n",
    "            AutoConfig.from_pretrained(args.model_name_or_path).vocab_size\n",
    "        )\n",
    "        \n",
    "        tokenizer.add_tokens(list(new_tokenizer.vocab.keys()))\n",
    "\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "    return RobertaTokenizerFast(tokenizer_file=TOKENIZER_PATH, model_max_length=args.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T22:59:32.480765Z",
     "iopub.status.busy": "2023-05-06T22:59:32.480541Z",
     "iopub.status.idle": "2023-05-06T22:59:32.487968Z",
     "shell.execute_reply": "2023-05-06T22:59:32.487264Z",
     "shell.execute_reply.started": "2023-05-06T22:59:32.480743Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_new_embeddings(model, side, tokenizer, strategy=\"avg\"):\n",
    "        \n",
    "    # based on https://nlp.stanford.edu//~johnhew//vocab-expansion.html\n",
    "\n",
    "    num_tokens = len(tokenizer)\n",
    "    \n",
    "    from transformers import AutoConfig\n",
    "        \n",
    "    num_new_tokens = len(tokenizer) - AutoConfig.from_pretrained(model.encoder.name_or_path).vocab_size\n",
    "\n",
    "    if side == \"encoder\":\n",
    "        model.encoder.resize_token_embeddings(num_tokens)\n",
    "        weight_key = 'encoder.embeddings.word_embeddings.weight'\n",
    "\n",
    "    else:\n",
    "        model.decoder.resize_token_embeddings(num_tokens)\n",
    "        weight_key = 'decoder.roberta.embeddings.word_embeddings.weight'\n",
    "\n",
    "    params = model.state_dict()\n",
    "\n",
    "    embeddings = params[weight_key]\n",
    "    \n",
    "    # embeddings = embeddings.to(torch.float32)\n",
    "\n",
    "    pre_expansion_embeddings = embeddings[:-num_new_tokens,:]\n",
    "    mu = torch.mean(pre_expansion_embeddings, dim=0)\n",
    "    n = pre_expansion_embeddings.size()[0]\n",
    "    sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n\n",
    "    dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "            mu, covariance_matrix=1e-5*sigma)\n",
    "\n",
    "    new_embeddings = torch.stack(tuple((dist.sample() for _ in range(num_new_tokens))), dim=0)\n",
    "    embeddings[-num_new_tokens:,:] = new_embeddings\n",
    "    \n",
    "    # embedings = embeddings.to(torch.float16)\n",
    "\n",
    "    params[weight_key] = embeddings\n",
    "\n",
    "    model.load_state_dict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T22:59:32.489092Z",
     "iopub.status.busy": "2023-05-06T22:59:32.488824Z",
     "iopub.status.idle": "2023-05-06T22:59:32.495341Z",
     "shell.execute_reply": "2023-05-06T22:59:32.494751Z",
     "shell.execute_reply.started": "2023-05-06T22:59:32.489069Z"
    }
   },
   "outputs": [],
   "source": [
    "# from https://huggingface.co/docs/transformers/v4.18.0/en/performance#faster-optimizer\n",
    "def get_optimizer(model, training_args):\n",
    "    import bitsandbytes as bnb\n",
    "    from torch import nn\n",
    "    from transformers.trainer_pt_utils import get_parameter_names\n",
    "\n",
    "    decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "            \"weight_decay\": training_args.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    optimizer_kwargs = {\n",
    "        \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
    "        \"eps\": training_args.adam_epsilon,\n",
    "    }\n",
    "    optimizer_kwargs[\"lr\"] = training_args.learning_rate\n",
    "    adam_bnb_optim = bnb.optim.Adam8bit(\n",
    "        optimizer_grouped_parameters,\n",
    "        betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
    "        eps=training_args.adam_epsilon,\n",
    "        lr=training_args.learning_rate,\n",
    "    )\n",
    "    \n",
    "    return adam_bnb_optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xT4n5ahHFHl4",
    "tags": []
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T22:59:32.496309Z",
     "iopub.status.busy": "2023-05-06T22:59:32.496076Z",
     "iopub.status.idle": "2023-05-06T22:59:32.503649Z",
     "shell.execute_reply": "2023-05-06T22:59:32.502969Z",
     "shell.execute_reply.started": "2023-05-06T22:59:32.496289Z"
    },
    "id": "gPcHJi6DATE_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 3 recommended for finetuning by BERT paper but 10 in CodeBERT example: https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_VARIANT = \"microsoft/codebert-base-mlm\"\n",
    "max_epochs = 5 \n",
    "\"\"\" 3 recommended for finetuning by BERT paper but 10 in CodeBERT example: https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T23:13:51.557927Z",
     "iopub.status.busy": "2023-05-06T23:13:51.557581Z",
     "iopub.status.idle": "2023-05-06T23:13:51.581215Z",
     "shell.execute_reply": "2023-05-06T23:13:51.580072Z",
     "shell.execute_reply.started": "2023-05-06T23:13:51.557900Z"
    },
    "id": "oFHkNpSK8fH_"
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    ## Required parameters  \n",
    "    parser.add_argument(\"--model_type\", default=None, type=str, required=True,\n",
    "                      help=\"Model type: e.g. roberta\")\n",
    "    parser.add_argument(\"--model_name_or_path\", default=None, type=str, required=True,\n",
    "                      help=\"Path to pre-trained model: e.g. roberta-base\" )\n",
    "    parser.add_argument(\"--output_dir\", default=None, type=str, required=True,\n",
    "                      help=\"The output directory. Contains any cached files or outputs\")\n",
    "\n",
    "    ## Other parameters\n",
    "    parser.add_argument(\"--example_dir\", default=None, type=str, \n",
    "                      help=\"The example directory. Contains source and test .py files\")\n",
    "    parser.add_argument(\"--max_length\", default=512, type=int,\n",
    "                      help=\"The maximum total target sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\")\n",
    "    parser.add_argument(\"--max_epochs\", default=-1, type=int,\n",
    "                        help=\"\")\n",
    "    parser.add_argument(\"--do_train\", action='store_true',\n",
    "                        help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--batch_size\", default=4, type=int,\n",
    "                      help=\"Batch size per GPU/CPU for training and evaluation.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
    "                      help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--num_beams\", default=10, type=int,\n",
    "                      help=\"The number of beams for beam search\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
    "                        help=\"Weight decay if we apply some.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
    "                      help=\"Epsilon for Adam optimizer.\")\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                      help=\"random seed for initialization\")\n",
    "    parser.add_argument('--subset_size', type=int, default=-1,\n",
    "                      help=\"Size of subset of dataset to use if not training on entirety\")\n",
    "    parser.add_argument('--report_to', type=str, default=\"none\",\n",
    "                      help=\"Where to log training data to\")\n",
    "    parser.add_argument('--warmup_ratio', type=str, default=0.05,\n",
    "                      help=\"Warmup ratio for linear scheduler; default based on 5 out of 90 epochs in original paper: https://arxiv.org/abs/1706.02677\")\n",
    "    \n",
    "    # print arguments\n",
    "    args = parser.parse_args() if len(args) == 0 else parser.parse_args(args)\n",
    "\n",
    "    # Dataset\n",
    "    \n",
    "    # first load dataset\n",
    "    dataset = load_and_save_dataset(args)\n",
    "    \n",
    "    # use dataset to initialise tokenizer\n",
    "    tokenizer = load_tokenizer(args, dataset)\n",
    "    tokenizer.bos_token = tokenizer.cls_token\n",
    "    tokenizer.eos_token = tokenizer.sep_token\n",
    "    \n",
    "    # use tokenizer to tranform and prep the dataset for training\n",
    "    dataset = prep_dataset(dataset, tokenizer)\n",
    "\n",
    "    # CodeBERT model config\n",
    "    \n",
    "    from transformers import EncoderDecoderModel, AutoModelForSeq2SeqLM\n",
    "    \n",
    "    ENCODER_DECODER_PATH = os.path.join(args.output_dir, \"encoder_decoder\")\n",
    "    \n",
    "    encoder_decoder = EncoderDecoderModel.from_encoder_decoder_pretrained(args.model_name_or_path, args.model_name_or_path, tie_encoder_decoder=True)\n",
    "    \n",
    "    # alter model encoder decoder embeddings using tokenizer\n",
    "    \n",
    "    for side in [\"encoder\", \"decoder\"]:\n",
    "        init_new_embeddings(encoder_decoder, side, tokenizer)\n",
    "    \n",
    "    encoder_decoder.save_pretrained(ENCODER_DECODER_PATH)\n",
    "    \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ENCODER_DECODER_PATH,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        decoder_start_token_id=tokenizer.bos_token_id,\n",
    "    )\n",
    "    \n",
    "    model.save_pretrained(args.output_dir)\n",
    "    \n",
    "    # Trainer\n",
    "    # args\n",
    "\n",
    "    from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, GenerationConfig\n",
    "    \n",
    "    gen_kwargs = {\n",
    "        'pad_token_id':tokenizer.pad_token_id,\n",
    "        'decoder_start_token_id':tokenizer.bos_token_id,\n",
    "        'max_new_tokens':args.max_length,\n",
    "        'min_new_tokens':args.max_length//2,\n",
    "        # beam-search multinomial sampling strategy\n",
    "        'do_sample':True,\n",
    "        'num_beams':args.num_beams,\n",
    "        'early_stopping':True,\n",
    "    }\n",
    "    \n",
    "    generation_config = GenerationConfig(**gen_kwargs)\n",
    "    \n",
    "    gradient_accumulation_steps = args.batch_size\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        fp16=True,\n",
    "        fp16_full_eval=True,\n",
    "        seed=args.seed,\n",
    "        save_total_limit=3,\n",
    "        report_to=args.report_to,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_bert\",\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        adam_epsilon=args.adam_epsilon,\n",
    "        num_train_epochs=args.max_epochs,\n",
    "        learning_rate=args.learning_rate,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        optim=\"adafactor\",\n",
    "        output_dir=f\"{args.output_dir}/trainer\",\n",
    "        per_device_train_batch_size=gradient_accumulation_steps//4,\n",
    "        per_device_eval_batch_size=gradient_accumulation_steps//4,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        gradient_checkpointing=True,\n",
    "        predict_with_generate=True,\n",
    "        generation_num_beams=args.num_beams,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    \n",
    "    # Data Collator\n",
    "    \n",
    "    from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        max_length=args.max_length,\n",
    "        padding=\"max_length\",\n",
    "        label_pad_token_id=model.config.pad_token_id,\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "    )\n",
    "    \n",
    "    # metrics\n",
    "    import evaluate\n",
    "    metric = evaluate.load(\"bertscore\")\n",
    "    \n",
    "    def decode_sequences(sequences):\n",
    "        return tokenizer.convert_tokens_to_string(tokenizer.batch_decode(sequences, skip_special_tokens=True))\n",
    "    \n",
    "    # adapted from https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation.py\n",
    "    \n",
    "    def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [[label.strip()] for label in labels]\n",
    "\n",
    "        return preds, labels\n",
    "\n",
    "    def compute_metrics(eval_preds, output_dir=args.output_dir):\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        # Replace -100s used for padding as we can't decode them\n",
    "        preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Some simple post-processing\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels, model_type=\"roberta-large\")\n",
    "        \n",
    "        return {\"eval_bert\": np.mean(result[\"f1\"]).item()}\n",
    "    \n",
    "    # training\n",
    "    \n",
    "    random.seed(a=args.seed)\n",
    "    \n",
    "    eval_ds = dataset[\"test\"]\n",
    "    eval_subset = eval_ds.select(random.sample(range(0, len(eval_ds)), len(eval_ds)//50))\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=eval_subset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=(get_optimizer(model, training_args), None)\n",
    "    )\n",
    "    \n",
    "    resume = len(os.listdir(os.path.join(args.output_dir, \"trainer\"))) > 0\n",
    "    \n",
    "    if args.do_train:\n",
    "        trainer.train(resume_from_checkpoint=resume)\n",
    "    else:\n",
    "        \n",
    "        inputs = dataset[\"train\"][0]\n",
    "        \n",
    "        text = decode_sequences(inputs[\"input_ids\"])\n",
    "        \n",
    "        preds, _, _ = trainer.predict([inputs])\n",
    "        \n",
    "        preds = decode_sequences(preds)\n",
    "        \n",
    "        print(f\"Input: {text}\")\n",
    "        \n",
    "        print(f\"Predictions: {preds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNCoz5ojMV3r"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b006ced0d0d747fe809275ded42efea5",
      "b9cd514194d648a9b07a116090bc705d",
      "bb4a1c1479e44c72900093747fdc4268",
      "231b983786864f89b4d40b6f94cff019",
      "27461e33792642ddb80e1acafd53dadd",
      "17ab6a84162b4039bdb38cacd1a3b942",
      "c82fd8da2bb84af6b7fe81043c1f72f9",
      "7a69b5f83bc44c2ca6e7bff01aa4acfd",
      "47ba75ed5b534e999bcac7762738e387",
      "ef253bad33f8473b995f69aca6c4d17e",
      "68389efdd5b14686a40b5dd64cc8f3c5",
      "54ec3217481d4a5da68106057f3d2756",
      "da2c00b651ec4e0baaebed267243f7b2",
      "ca2803a846e24dde9baed99d885d1348",
      "fead1800ddd54206bc37cc5d005a4e1b",
      "39c91516bfaf49faa69bfbba241832f4",
      "ec496d0254e1484d80a33743ede5295c",
      "bed627199a4e41c6a5b67f305cda5290",
      "1481abaa12b14026aa60cad3fb2d16b3",
      "7f3807584a6a43aab691a5d25ba2de87",
      "1aa3a79b6b8e4ade8b074b42a7b0e64c",
      "c31cfd562ef4402dbc3cc4e5a2e57b0e"
     ]
    },
    "execution": {
     "iopub.execute_input": "2023-05-06T22:59:32.526732Z",
     "iopub.status.busy": "2023-05-06T22:59:32.526517Z",
     "iopub.status.idle": "2023-05-06T22:59:32.530135Z",
     "shell.execute_reply": "2023-05-06T22:59:32.529422Z",
     "shell.execute_reply.started": "2023-05-06T22:59:32.526710Z"
    },
    "id": "32nPVZni9z65",
    "outputId": "4c2b418c-e1fa-4076-dab5-1b4e15c7d5d3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main([\n",
    "#     \"--model_type\", \"roberta\", \n",
    "#     \"--model_name_or_path\", MODEL_VARIANT,\n",
    "#     \"--example_dir\", f\"{DATA_DIR}\",\n",
    "#     \"--output_dir\", f\"{TEST_BERT_DIR}\",\n",
    "#     \"--max_epochs\", f\"{max_epochs}\",\n",
    "#     \"--report_to\", \"neptune\",\n",
    "#     \"--do_train\"\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0TfgZaW2PB2",
    "tags": []
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-06T23:14:02.385465Z",
     "iopub.status.busy": "2023-05-06T23:14:02.385084Z",
     "iopub.status.idle": "2023-05-06T23:14:45.963272Z",
     "shell.execute_reply": "2023-05-06T23:14:45.962479Z",
     "shell.execute_reply.started": "2023-05-06T23:14:02.385437Z"
    },
    "id": "_we9rUiW2Ouq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /notebooks/TestBERT/datasets/cache-e8a9d47c83606666.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /notebooks/TestBERT/datasets/cache-ed329f0c9ca20592.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /notebooks/TestBERT/datasets/cache-7b586e7b0668e2ca.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /notebooks/TestBERT/datasets/cache-d460c2d3bd1949d1.arrow\n",
      "Loading cached split indices for dataset at /notebooks/TestBERT/datasets/cache-7d5b287785bdc275.arrow and /notebooks/TestBERT/datasets/cache-23d4ea0bb717758c.arrow\n",
      "Some weights of the model checkpoint at microsoft/codebert-base-mlm were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at microsoft/codebert-base-mlm and are newly initialized: ['roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/usr/lib/python3.9/logging/__init__.py\", line 663, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/usr/lib/python3.9/logging/__init__.py\", line 367, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/traitlets/config/application.py\", line 1041, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2885, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3139, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3318, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2343/143171260.py\", line 1, in <module>\n",
      "    main([\"--model_type\", \"roberta\", \"--model_name_or_path\", MODEL_VARIANT, \"--output_dir\", f\"{TEST_BERT_DIR}\"])\n",
      "  File \"/tmp/ipykernel_2343/2586047677.py\", line 197, in main\n",
      "    preds, _, _ = trainer.predict([inputs])\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer_seq2seq.py\", line 216, in predict\n",
      "    return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 3069, in predict\n",
      "    output = eval_loop(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\", line 3174, in evaluation_loop\n",
      "    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/trainer_seq2seq.py\", line 271, in prediction_step\n",
      "    generated_tokens = self.model.generate(**inputs, **gen_kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py\", line 1322, in generate\n",
      "    logger.warn(\n",
      "Message: 'Both `max_new_tokens` (=512) and `max_length`(=513) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <s>url_segment(connector_id).delete().go()\n",
      "\n",
      "    def delete_consent(self, consent_id):\n",
      "        return self.start().uri('/api/consent').url_segment(consent_id).delete().go()\n",
      "\n",
      "    def delete_email_template(self, email_template_id):\n",
      "        return self.start().uri('/api/email/template').url_segment(email_template_id).delete().go()\n",
      "\n",
      "    def delete_entity(self, entity_id):\n",
      "        return self.start().uri('/api/entity').url_segment(entity_id).delete().go()\n",
      "\n",
      "    def delete_entity_grant(self, entity_id, recipient_entity_id=None, user_id=None):\n",
      "        return self.start().uri('/api/entity').url_segment(entity_id).url_segment('grant').url_parameter('recipientEntityId', self.convert_true_false(recipient_entity_id)).url_parameter('userId', self.convert_true_false(user_id)).delete().go()\n",
      "\n",
      "    def delete_entity_type(self, entity_type_id):\n",
      "        return self.start().uri('/api/entity/type').url_segment(entity_type_id).delete().go()\n",
      "\n",
      "    def delete_entity_type_permission(self, entity_type_id, permission_id):\n",
      "        return self.start().uri('/api/entity/type').url_segment(entity_type_id).url_segment('permission').url_segment(permission_id).delete().go()\n",
      "\n",
      "    def delete_form(self, form_id):\n",
      "        return self.start().uri('/api/form').url_segment(form_id).delete().go()\n",
      "\n",
      "    def</s>\n",
      "Predictions: <s>p as as as as as as – as as as as – ass – – – –ss – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –� – – – – – –� – – – – – – – – – – – – – – – – – – – – –� – – – – –� – – –�� – – – –� –� –� –��� –�� –��� –�f�f��f –��f���f�� –f�s�s��s�...s��... –s�s�s –f�s�ssgggsggggggggssggsggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggGGgggggggggggggggGgggGgggggGgggggggGggGgggGggGgGgGGggggggggGgggGGgGGGggggggGgGgggGGGGgggGggggggGgGggggggGGggggggggggggGGggGGggggggggggggGgggggGgggGGgggggggggggggGgggGgGggggggggGggggggggggggggggggggggggggggGgggggggGgggggGggggggGggggggggggggggggggggggggg\n"
     ]
    }
   ],
   "source": [
    "# main([\"--model_type\", \"roberta\", \"--model_name_or_path\", MODEL_VARIANT, \"--output_dir\", f\"{TEST_BERT_DIR}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-06T23:00:23.598902Z",
     "iopub.status.idle": "2023-05-06T23:00:23.599215Z",
     "shell.execute_reply": "2023-05-06T23:00:23.599081Z",
     "shell.execute_reply.started": "2023-05-06T23:00:23.599064Z"
    }
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "gQeiHGSpEAfh",
    "TC-mTkN-C59W",
    "gmeEF9SaDSWn",
    "Nti5WlziPv7o"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1481abaa12b14026aa60cad3fb2d16b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17ab6a84162b4039bdb38cacd1a3b942": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1aa3a79b6b8e4ade8b074b42a7b0e64c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "231b983786864f89b4d40b6f94cff019": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef253bad33f8473b995f69aca6c4d17e",
      "placeholder": "​",
      "style": "IPY_MODEL_68389efdd5b14686a40b5dd64cc8f3c5",
      "value": " 0/? [00:00&lt;?, ?it/s]"
     }
    },
    "27461e33792642ddb80e1acafd53dadd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "39c91516bfaf49faa69bfbba241832f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "47ba75ed5b534e999bcac7762738e387": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "54ec3217481d4a5da68106057f3d2756": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da2c00b651ec4e0baaebed267243f7b2",
       "IPY_MODEL_ca2803a846e24dde9baed99d885d1348",
       "IPY_MODEL_fead1800ddd54206bc37cc5d005a4e1b"
      ],
      "layout": "IPY_MODEL_39c91516bfaf49faa69bfbba241832f4"
     }
    },
    "68389efdd5b14686a40b5dd64cc8f3c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a69b5f83bc44c2ca6e7bff01aa4acfd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f3807584a6a43aab691a5d25ba2de87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b006ced0d0d747fe809275ded42efea5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b9cd514194d648a9b07a116090bc705d",
       "IPY_MODEL_bb4a1c1479e44c72900093747fdc4268",
       "IPY_MODEL_231b983786864f89b4d40b6f94cff019"
      ],
      "layout": "IPY_MODEL_27461e33792642ddb80e1acafd53dadd"
     }
    },
    "b9cd514194d648a9b07a116090bc705d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_17ab6a84162b4039bdb38cacd1a3b942",
      "placeholder": "​",
      "style": "IPY_MODEL_c82fd8da2bb84af6b7fe81043c1f72f9",
      "value": "Sanity Checking: "
     }
    },
    "bb4a1c1479e44c72900093747fdc4268": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a69b5f83bc44c2ca6e7bff01aa4acfd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_47ba75ed5b534e999bcac7762738e387",
      "value": 0
     }
    },
    "bed627199a4e41c6a5b67f305cda5290": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c31cfd562ef4402dbc3cc4e5a2e57b0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c82fd8da2bb84af6b7fe81043c1f72f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca2803a846e24dde9baed99d885d1348": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1481abaa12b14026aa60cad3fb2d16b3",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7f3807584a6a43aab691a5d25ba2de87",
      "value": 0
     }
    },
    "da2c00b651ec4e0baaebed267243f7b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec496d0254e1484d80a33743ede5295c",
      "placeholder": "​",
      "style": "IPY_MODEL_bed627199a4e41c6a5b67f305cda5290",
      "value": "Epoch 0:   0%"
     }
    },
    "ec496d0254e1484d80a33743ede5295c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef253bad33f8473b995f69aca6c4d17e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fead1800ddd54206bc37cc5d005a4e1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1aa3a79b6b8e4ade8b074b42a7b0e64c",
      "placeholder": "​",
      "style": "IPY_MODEL_c31cfd562ef4402dbc3cc4e5a2e57b0e",
      "value": " 0/1 [00:00&lt;?, ?it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
